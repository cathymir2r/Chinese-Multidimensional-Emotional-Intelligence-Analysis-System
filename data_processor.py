# sentiment_analyzer/data_processor.py
import pandas as pd
import re
import jieba
import os

# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
def load_data(train_path, dev_path=None, test_path=None):
    """
    åŠ è½½æ•°æ®é›†
    """
    # å¦‚æœåªæä¾›äº†ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰
    if dev_path is None and test_path is None:
        try:
            # ä½¿ç”¨ä¼ å…¥çš„è·¯å¾„å‚æ•°
            data = pd.read_csv(train_path, sep='\t')
        except:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
            data = pd.read_csv(train_path, sep=None, engine='python')
            
        if 'label' not in data.columns and 'text_a' not in data.columns:
            # å°è¯•æ£€æµ‹å¹¶è°ƒæ•´åˆ—å
            if len(data.columns) == 2:
                data.columns = ['label', 'text_a']
            elif 'text' in data.columns:
                data['text_a'] = data['text']
                if 'label' not in data.columns:
                    data['label'] = -1  # é»˜è®¤æ ‡ç­¾
        
        print(f"æ•°æ®é›†å¤§å°: {len(data)}")
        return data
    
    # å¤šæ–‡ä»¶æ¨¡å¼ï¼ˆè®­ç»ƒé›†+éªŒè¯é›†+æµ‹è¯•é›†ï¼‰
    try:
        train_data = pd.read_csv(train_path, sep='\t')
    except:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
        train_data = pd.read_csv(train_path, sep=None, engine='python')
    
    # å¤„ç†éªŒè¯é›†
    if dev_path:
        try:
            dev_data = pd.read_csv(dev_path, sep='\t')
        except:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
            dev_data = pd.read_csv(dev_path, sep=None, engine='python')
    else:
        # å¦‚æœæ²¡æœ‰æä¾›éªŒè¯é›†ï¼Œåˆ›å»ºç©ºçš„DataFrame
        dev_data = pd.DataFrame(columns=['qid', 'label', 'text_a'])
    
    # å¤„ç†æµ‹è¯•é›†
    if test_path:
        try:
            test_data = pd.read_csv(test_path, sep='\t')
        except:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
            test_data = pd.read_csv(test_path, sep=None, engine='python')
    else:
        # å¦‚æœæ²¡æœ‰æä¾›æµ‹è¯•é›†ï¼Œåˆ›å»ºç©ºçš„DataFrame
        test_data = pd.DataFrame(columns=['qid', 'text_a', 'label'])
    
    # æ ¹æ®æä¾›çš„æ•°æ®é›†æ ¼å¼è°ƒæ•´åˆ—å
    if 'label' in train_data.columns and 'text_a' in train_data.columns:
        # è®­ç»ƒé›†æ ¼å¼æ­£ç¡®
        pass
    else:
        # è°ƒæ•´è®­ç»ƒé›†åˆ—å
        if len(train_data.columns) >= 2:
            train_data.columns = ['label', 'text_a'] + list(train_data.columns[2:])
    
    # å¤„ç†éªŒè¯é›†åˆ—å
    if len(dev_data) > 0:  # åªæœ‰å½“éªŒè¯é›†ä¸ä¸ºç©ºæ—¶æ‰å¤„ç†
        if 'qid' in dev_data.columns and 'label' in dev_data.columns and 'text_a' in dev_data.columns:
            # éªŒè¯é›†æ ¼å¼æ­£ç¡®
            pass
        else:
            # è°ƒæ•´éªŒè¯é›†åˆ—å
            if len(dev_data.columns) >= 3:
                dev_data.columns = ['qid', 'label', 'text_a'] + list(dev_data.columns[3:])
            elif len(dev_data.columns) >= 2:
                dev_data.columns = ['label', 'text_a'] + list(dev_data.columns[2:])
                dev_data.insert(0, 'qid', range(len(dev_data)))
    
    # å¤„ç†æµ‹è¯•é›†åˆ—å
    if len(test_data) > 0:  # åªæœ‰å½“æµ‹è¯•é›†ä¸ä¸ºç©ºæ—¶æ‰å¤„ç†
        if 'qid' in test_data.columns and 'text_a' in test_data.columns:
            # æµ‹è¯•é›†æ ¼å¼æ­£ç¡®
            pass
        else:
            # è°ƒæ•´æµ‹è¯•é›†åˆ—å
            if len(test_data.columns) >= 2:
                test_data.columns = ['qid', 'text_a'] + list(test_data.columns[2:])
            elif len(test_data.columns) >= 1:
                test_data.columns = ['text_a'] + list(test_data.columns[1:])
                test_data.insert(0, 'qid', range(len(test_data)))
        
        # å¦‚æœæµ‹è¯•é›†æ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ ‡ç­¾ï¼ˆ-1ï¼‰ç”¨äºåç»­å¤„ç†
        if 'label' not in test_data.columns:
            test_data['label'] = -1
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}")
    print(f"éªŒè¯é›†å¤§å°: {len(dev_data)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
    
    # å§‹ç»ˆè¿”å›ä¸‰ä¸ªæ•°æ®é›†
    return train_data, dev_data, test_data


# 2. æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†
def clean_text(text):
    """
    æ¸…æ´—æ–‡æœ¬æ•°æ®
    """
    if isinstance(text, str):
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<.*?>', '', text)
        # å»é™¤URL
        text = re.sub(r'http\S+', '', text)
        # å»é™¤æ•°å­—
        text = re.sub(r'\d+', '', text)
        # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', '', text)
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    else:
        return ""

def tokenize(text):
    """
    ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    """
    return list(jieba.cut(text))

def preprocess_data(data):
    """
    å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
    """
    # åˆ›å»ºæ•°æ®å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    data = data.copy()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰text_aåˆ—
    if 'text_a' not in data.columns:
        print("è­¦å‘Š: æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ° 'text_a' åˆ—")
        return data
    
    # æ¸…æ´—æ–‡æœ¬
    data['clean_text'] = data['text_a'].apply(clean_text)
    
    # åˆ†è¯
    data['tokens'] = data['clean_text'].apply(tokenize)
    
    return data

def split_data(data, test_size=0.2, dev_size=0.1, random_state=42):
    """
    å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    """
    from sklearn.model_selection import train_test_split
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(data) == 0:
        print("è­¦å‘Š: è¾“å…¥æ•°æ®ä¸ºç©º")
        return data, pd.DataFrame(), pd.DataFrame()
    
    # å…ˆåˆ†ç¦»å‡ºæµ‹è¯•é›†
    train_dev_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)
    
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ†ç¦»å‡ºéªŒè¯é›†
    if dev_size > 0 and len(train_dev_data) > 1:
        train_data, dev_data = train_test_split(train_dev_data, test_size=dev_size/(1-test_size), random_state=random_state)
    else:
        train_data = train_dev_data
        dev_data = pd.DataFrame()
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}")
    print(f"éªŒè¯é›†å¤§å°: {len(dev_data)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
    
    return train_data, dev_data, test_data

def get_data_statistics(data):
    """
    è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    """
    if len(data) == 0:
        return {"message": "æ•°æ®é›†ä¸ºç©º"}
    
    stats = {}
    stats['total_samples'] = len(data)
    stats['columns'] = list(data.columns)
    
    # æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
    if 'label' in data.columns:
        label_counts = data['label'].value_counts().sort_index()
        stats['label_distribution'] = label_counts.to_dict()
    
    # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
    if 'text_a' in data.columns:
        text_lengths = data['text_a'].astype(str).str.len()
        stats['text_length'] = {
            'mean': text_lengths.mean(),
            'std': text_lengths.std(),
            'min': text_lengths.min(),
            'max': text_lengths.max()
        }
    
    return stats

def print_data_statistics(data, dataset_name="æ•°æ®é›†"):
    """
    æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"\nğŸ“Š {dataset_name} ç»Ÿè®¡ä¿¡æ¯:")
    print("=" * 50)
    
    if len(data) == 0:
        print("æ•°æ®é›†ä¸ºç©º")
        return
    
    stats = get_data_statistics(data)
    
    print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"åˆ—å: {', '.join(stats['columns'])}")
    
    if 'label_distribution' in stats:
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in stats['label_distribution'].items():
            percentage = count / stats['total_samples'] * 100
            print(f"  æ ‡ç­¾ {label}: {count} æ¡ ({percentage:.1f}%)")
    
    if 'text_length' in stats:
        text_stats = stats['text_length']
        print(f"\næ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {text_stats['mean']:.1f} å­—ç¬¦")
        print(f"  æ ‡å‡†å·®: {text_stats['std']:.1f}")
        print(f"  æœ€çŸ­: {text_stats['min']} å­—ç¬¦")
        print(f"  æœ€é•¿: {text_stats['max']} å­—ç¬¦")

def validate_data_format(data, dataset_type='train'):
    """
    éªŒè¯æ•°æ®æ ¼å¼
    """
    if len(data) == 0:
        print(f"âš ï¸ {dataset_type}æ•°æ®é›†ä¸ºç©º")
        return False
    
    required_columns = ['text_a']
    missing_columns = [col for col in required_columns if col not in data.columns]
    
    if missing_columns:
        print(f"âŒ {dataset_type}æ•°æ®é›†ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
        return False
    
    # æ£€æŸ¥ç©ºå€¼
    null_count = data['text_a'].isnull().sum()
    if null_count > 0:
        print(f"âš ï¸ {dataset_type}æ•°æ®é›†ä¸­æœ‰ {null_count} ä¸ªç©ºå€¼")
    
    print(f"âœ… {dataset_type}æ•°æ®é›†æ ¼å¼éªŒè¯é€šè¿‡")
    return True


# 3. ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®¾ç½®æ•°æ®æ–‡ä»¶è·¯å¾„
    train_path = r"C:\Users\cathy\Desktop\sentiment_analysis_project\data\train.tsv"
    dev_path = r"C:\Users\cathy\Desktop\sentiment_analysis_project\data\dev.tsv"
    test_path = r"C:\Users\cathy\Desktop\sentiment_analysis_project\data\test.tsv"
    
    # åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    train_data, dev_data, test_data = load_data(train_path, dev_path, test_path)
    
    # éªŒè¯æ•°æ®æ ¼å¼
    validate_data_format(train_data, 'train')
    if len(dev_data) > 0:
        validate_data_format(dev_data, 'dev')
    if len(test_data) > 0:
        validate_data_format(test_data, 'test')
    
    # é¢„å¤„ç†æ•°æ®
    print("\næ­£åœ¨é¢„å¤„ç†è®­ç»ƒæ•°æ®...")
    train_data = preprocess_data(train_data)
    
    if len(dev_data) > 0:
        print("æ­£åœ¨é¢„å¤„ç†éªŒè¯æ•°æ®...")
        dev_data = preprocess_data(dev_data)
    
    if len(test_data) > 0:
        print("æ­£åœ¨é¢„å¤„ç†æµ‹è¯•æ•°æ®...")
        test_data = preprocess_data(test_data)
    
    # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
    print("\nè®­ç»ƒæ•°æ®æ ·ä¾‹:")
    print(train_data.head())
    
    if len(dev_data) > 0:
        print("\néªŒè¯æ•°æ®æ ·ä¾‹:")
        print(dev_data.head())
    
    if len(test_data) > 0:
        print("\næµ‹è¯•æ•°æ®æ ·ä¾‹:")
        print(test_data.head())
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print_data_statistics(train_data, "è®­ç»ƒé›†")
    if len(dev_data) > 0:
        print_data_statistics(dev_data, "éªŒè¯é›†")
    if len(test_data) > 0:
        print_data_statistics(test_data, "æµ‹è¯•é›†")